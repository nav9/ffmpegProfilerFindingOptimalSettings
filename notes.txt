Duration:
-----------------------------
Researching: 20 Oct 2022
Design: 21 Oct 2022

Details:
-----------------------------
ffmpeg version 2.8.17-0ubuntu0.1

Reference:
-----------------------------
* FFMPEG man page `man ffmpeg`.
* Video quality measurement (old): https://en.wikipedia.org/wiki/Visual_information_fidelity (python package: https://pypi.org/project/sewar/)
* More video quality measurements (and benchmark code): https://piq.readthedocs.io/en/latest/overview.html
* FFMPEG profiling tool (contains only Docker containers): https://github.com/mboussaa/ffmpeg-profiling-tool
* Timeseries database for storing metrics: https://www.influxdata.com/influxdb-pricing/
* Observability of databases: https://grafana.com/
* Commercial encoding: https://www.telestream.net/
* pyTranscoder for FFMPEG: https://pypi.org/project/pytranscoder-ffmpeg/
* Linux app profiling: https://stackoverflow.com/questions/2229336/linux-application-profiling
* FFMPEG quality cheatsheets: https://superuser.com/questions/490683/cheat-sheets-and-presets-settings-that-actually-work-with-ffmpeg-1-0
* Explanation: https://medium.com/videocoin/what-is-video-transcoding-and-why-do-you-do-it-348a2610cefc
* Effect of CRF on Youtube video quality: https://www.dropbox.com/s/fsxew599dw30c5w/Report.docx?dl=0
* Film making terms glossary: https://www.studiobinder.com/blog/movie-film-terms/
* Visual information fidelity: https://github.com/pavancm/Visual-Information-Fidelity---Python
* Objective video quality assessment: Zhou Wang, Hamid R. Sheikh and Alan C. Bovik. Chapter 41 in The Handbook of Video Databases: Design and Applications, B. Furht and O. Marqure, ed., CRC Press, pp. 1041-1078, September 2003
* Netflix / vmaf: Video Multi-Method Assessment Fusion
* Capped CRF: https://streaminglearningcenter.com/encoding/saving-encoding-streaming-deploy-capped-crf.html
* NVIDIA transcoding guide: https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/
* Process info (from Python): https://stackoverflow.com/questions/16326529/python-get-process-names-cpu-mem-usage-and-peak-mem-usage-in-windows
* ps CPU time measurement vs. top: https://unix.stackexchange.com/questions/460832/how-does-ps-measure-cpu-per-process-and-can-this-be-changed
* CPU time using atop and SAR: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-monitor-stats-with-atop/
* Various profilers: https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process
* More CPU usage: https://stackoverflow.com/questions/1221555/retrieve-cpu-usage-and-memory-usage-of-a-single-process-on-linux
* Optimal bit rate: https://teradek.com/blogs/articles/what-is-the-optimal-bitrate-for-your-resolution
* Bit rate tables: https://filmora.wondershare.com/video-editing-tips/what-is-video-bitrate.html
* Artifact: https://en.wikipedia.org/wiki/Artifact_(error). https://en.wikipedia.org/wiki/Ringing_artifacts
* Post processing filter: https://trac.ffmpeg.org/wiki/Postprocessing (Postprocessing is usually only useful with older codecs. Newer ones (including h.264, h.265, and VP8/9) all include deblocking filters as part of the codec. Most of the time it won't help to postprocess h.264, HEVC, VP8, or VP9 video. Postprocessing IS certainly useful for artifacty MPEG2 and MPEG4-ASP (divx, xvid), and even older codecs, as their output is often full of blocking and ringing when the encoder didn't have enough bitrate to make the output look good http://underpop.online.fr/f/ffmpeg/help/deblock.htm.gz)

Terms (sourced from the sites referenced above):
-----------------------------
* Transcoding (which is a process of decoding, reformatting and re-encoding files) takes source footage of various types and recodes it into a single video codec or file format. Video transcoding, sometimes called video encoding, is the conversion from one digital encoding format to another. This involves translating all three elements of a digital video at the same time — the file format, the video, and the audio. It involves a two-step process. First, the original file is decoded to an uncompressed format. Second, this uncompressed format is then encoded into the target format.
* Video format specifies how video and audio have been combined and tells playback devices how to play the files.
* Video and audio codec formats refer to the technologies used to both create and play back the digital video and audio. DaCast, for example, requires an MPEG-4 (MP4) file format with an H.264 video codec. Windows Media Video (WMV) files are Advanced Systems Format (.asf) files that include audio, video, or both compressed with Windows Media Audio (WMA) and Windows Media Video (WMV) codecs.
* CRF: Constant rate factor (CRF) is an encoding mode that adjusts the file data rate up or down to achieve a selected quality level rather than a specific data rate. CRF values range from 0 to 51, with lower numbers delivering higher quality scores. Multiple codecs support CRF, including x264, x265, and VP9.
* A keyframe, also written as “key frame,” is something that defines the starting and/or ending point of any smooth transition. That something can be a drawing in animation or a particular frame of a shot when dealing with film or video. Any shot, animated or live-action, is broken down into individual frames. You can think of keyframes as the most important frames of a shot that set the parameters for the other frames and indicate the changes that will occur throughout as transitions. For more information, refer to our filmmaker’s guide to frame rates. Keyframe Characteristics: 1. Important individual frames from within a shot; 2. Sets a start/stop point for a transition. Because of the heavy demands and time-consuming nature of animation, those films are typically made by a number of artists working together. One efficient way to both save time and ensure quality is to have the lead animators draw the most important frames and leave the transitional frames between them to the junior animators. These important frames drawn by the lead animators became known as keyframes. The transitional frames that connected the various keyframes together become known as in-betweens. As used in the context of video editing, keyframes might set the parameters for motion, they might be used to fine-tune a video transition. Or they might control timed adjustments made to effects applied to a video. The editing software can fill in all of those pesky in-betweens automatically.
* Capped CRF: On its own, CRF is unusable for adaptive bitrate streaming, where data rates in the ladder rungs need to be limited. However, by adding a “cap” to CRF, you limit the data rate to that cap. An FFmpeg argument implementing capped CRF would look like this: `ffmpeg -i input_file -crf 23 -maxrate 6750k -bufsize 6750k output_file`. This tells FFmpeg to encode at a quality level of 23, but to cap the data rate at 6750 kbps with a VBV buffer of 4500 kbps. For easy-to-encode clips, the CRF value would limit the data rate, as the required quality could be achieved at data rates lower than the cap. For hard-to-encode clips, the cap would kick in to control the data rate.
* Block-ing: As the compression increases, there's more possibility of artifacts. The blocky artifact you see above (and in the images below) is called macroblocking. It's an artifact that happens when the video codec (encoder/decoder, like MPEG) can't handle the amount of information being thrown at it, at the chosen bit rate, and the result is blocks in the image. Why blocks? It's a result of how the image is compressed in the codec. You'll most commonly see macroblocking in fast motion, or when there's a lot going on in the image. The confetti at the end of the Superbowl was a macroblocking mess. Shots of the ocean are another problem, as there are a lot of individual things moving in the shot (like the wave tops). The macroblock is a processing unit in image and video compression formats based on linear block transforms, typically the discrete cosine transform (DCT). A macroblock typically consists of 16×16 samples, and is further subdivided into transform blocks, and may be further subdivided into prediction blocks. Formats which are based on macroblocks include JPEG, where they are called MCU blocks, H.261, MPEG-1 Part 2, H.262/MPEG-2 Part 2, H.263, MPEG-4 Part 2, and H.264/MPEG-4 AVC. In H.265/HEVC, the macroblock as a basic processing unit has been replaced by the coding tree unit.
* A deblocking filter is a video filter applied to decoded compressed video to improve visual quality and prediction performance by smoothing the sharp edges which can form between macroblocks when block coding techniques are used. The filter aims to improve the appearance of decoded pictures. 
* pixelation (or pixellation in British English) is caused by displaying a bitmap or a section of a bitmap at such a large size that individual pixels, small single-colored square display elements that comprise the bitmap, are visible. Such an image is said to be pixelated.
* A compression artifact (or artefact) is a noticeable distortion of media (including images, audio, and video) caused by the application of lossy compression. 
* In natural science and signal processing, an artifact or artefact[1] is any error in the perception or representation of any information introduced by the involved equipment or technique(s)
* Transmission errors: Data errors in the compressed bit-stream, possibly due to transmission errors, can lead to errors similar to large quantization errors, or can disrupt the parsing of the data stream entirely for a short time, leading to "break-up" of the picture. 
* Block boundary discontinuities can occur at edges of motion compensation prediction blocks. In motion compensated video compression, the current picture is predicted by shifting blocks (macroblocks, partitions, or prediction units) of pixels from previously decoded frames. If two neighboring blocks use different motion vectors, there will be a discontinuity at the edge between the blocks. 
* Mosquito noise: Block boundary discontinuities can occur at edges of motion compensation prediction blocks. In motion compensated video compression, the current picture is predicted by shifting blocks (macroblocks, partitions, or prediction units) of pixels from previously decoded frames. If two neighboring blocks use different motion vectors, there will be a discontinuity at the edge between the blocks. 
* Ringing artifact: In signal processing, particularly digital image processing, ringing artifacts are artifacts that appear as spurious signals near sharp transitions in a signal. Visually, they appear as bands or "ghosts" near edges; audibly, they appear as "echos" near transients, particularly sounds from percussion instruments; most noticeable are the pre-echos. 
* Encoding only audio: `ffmpeg -i in.mp4 -c:v copy -c:a aac out.mp4`. Video is only copied as-is. Audio is encoded. (https://superuser.com/questions/1196107/ffmpeg-low-cpu-usage)
* Classification of objective video quality models: 1. Full Reference Methods (FR): FR models compute the quality difference by comparing
the original video signal against the received video signal. Typically, every pixel from the
source is compared against the corresponding pixel at the received video. 2. Reduced Reference Methods (RR): RR models extract some features of both videos and compare them to give a quality score. They are used when all the original video is not available, or when it would be practically impossible to do so, e.g. in a transmission with a limited bandwidth. 3. No-Reference Methods (NR): NR models try to assess the quality of a distorted video without any reference to the original signal ((a)Pixel-Based Methods (NR-P): Pixel-based models use a decoded representation of the signal and analyze the quality based on the pixel information. Some of these evaluate specific degradation types only, such as blurring or other coding artifacts. (b) Parametric/Bitstream Methods (NR-B): These models make use of features extracted from the transmission container and/or video bitstream, e.g. MPEG-TS packet headers, motion vectors and quantization parameters. They do not have access to the original signal and require no decoding of the video, which makes them more efficient. In contrast to NR-P models, they have no access to the final decoded signal. However, the picture quality predictions they deliver are not very accurate. (c) Hybrid Methods (Hybrid NR-P-B): Hybrid models combine parameters extracted from the bitstream with a decoded video signal. They are therefore a mix between NR-P and NR-B models).
* Bitrate is the number of bits per second. A video bitrate is the number of bits that are processed in a unit of time. The symbol is bit/s. It generally determines the size and quality of video and audio files: the higher the bitrate, the better the quality, and the larger the file size because of File size = bitrate (kilobits per second) x duration. In most cases, 1 byte per second (1 B/s) corresponds to 8 bit/s.
* How to find the optimal resolution to bitrate ratio? The truth is, there really is no right or wrong answer. Depending on your encoder, video content, audience, and streaming destination, your resolution and bitrate will likely be different. Key Considerations for Choosing a Resolution and Bitrate: 1. How much bandwidth do I have to broadcast? Dedicate 50% of your upload speed to your stream in case your connection suddenly becomes unstable or you encounter unexpected interference from sending across the internet to the streaming platform. If you have limited bandwidth at your broadcast site but your encoder is capable of streaming with a more efficient video codec like HEVC / H.265 instead of just H.264, you can send out a lower bitrate HEVC stream from your encoder and have it transcoded to H.264 in the cloud for final delivery to streaming destinations and viewers. 2. How is my audience watching? How much bandwidth do they have? Consider where and how your audience will be watching your video and what their bandwidth limitations may be. Are they on computers or mobile phones? Using Ethernet or LTE? If the majority of your audience doesn’t have enough bandwidth to watch your video in 1080p, then you shouldn’t have to push your bandwidth over the edge just to upload a 1080p video. Cloud transcoding can help those watching with slower connections or on mobile devices with data limits because it enables the single stream sent out from your encoder to be converted into several lower resolution and lower bitrate streams that the viewers can choose instead of each viewer receiving the same higher resolution / higher bitrate stream. 3. What type of content am I sending? How much do I need to send? More dynamic content requires higher bitrates to have good quality, so you will need a higher bitrate to stream sporting events or video game competitions as opposed to speakers giving presentations at a conference or commencement ceremonies. The encoded frame rate also affects the required bitrate. When streaming sports you could encode a 60 frames per second (fps) stream at 1080p60 or 720p60; for lower motion events like lectures or conferences, encoding and sending 60 fps may not provide a visible benefit, but requires significantly more bandwidth than streaming at the more common 30 fps. Not all online streaming platforms can accept a 60 fps stream, and not all viewers can handle receiving 60 fps, so be sure to take that into consideration. Finally, there are two different methods of encoding the video: constant and variable. A constant bitrate encodes at approximately the same rate throughout the stream. While this can help maintain consistent quality, a constant bitrate isn’t always ideal for streaming over the internet since the same amount of data is being sent even when the content isn’t very complex, incurring higher costs from mobile data plans. 4. To which CDN / OVP am I streaming? If you're streaming to a single destination - direct to Facebook Live or YouTube Live, for instance - then your encoder’s settings need to match that destination’s requirements for bitrate and resolution on the incoming stream. If you're using Core or Sharelink to stream to multiple destinations at the same time, then you need to choose encoding settings that are compatible with every destination, since the encoder will only send out a single stream into Core or Sharelink, and that stream is then replicated to each destination. Keep in mind that every streaming platform comes with different presets which may limit the video bitrate and resolution combinations they will accept.
* What’s the difference between Constant Bitrate (CBR) and Variable Bitrate (VBR) Encoding. Constant bitrate (CBR) encoding maintains a set bitrate over the entire video clip, but limits the image quality in most cases—especially for complex video segments. CBR is often not an optimal choice for streaming since it does not allocate enough data for the complex segments: this results in lower quality overall and unused capacity on the simple segments. Therefore, we recommend you avoid using CBR unless you have a specific requirement.


Improvements:
---------------------------
* Asking about the end user or use case, to be able to choose the right parameters for encoding.
* Hiring video consultants for some training to build in-house expertise.
* Trying software like Sorenson and others, to check for better encoding capability.
* Building in-house knowledge by automating software that tries out various parameters and outcomes.

