Duration:
---
Researching: 20 Oct 2022
Design & implementation: 21 Oct 2022 - 24 Oct 2022
Trials: 25 Oct 2022

Objectives:
---
* Main goal is to understand how to minimize CPU usage.
* Baseline benchmarking of the CPU usage percentage given by the OS (using utils like top, htop, etc) is fine to consider as a raw metric (which might include the impact of the sub parameters) to keep the scope in check and not go into a bigger loop. I/O is an issue, but is not a main bottleneck; it can be resolved by using SSD or NVME storage in AWS.

Details:
---
ffmpeg version 2.8.17-0ubuntu0.1

Reference:
---
* Sample videos: https://filesamples.com/formats/mkv
* FFMPEG man page `man ffmpeg`.
* Encoding on x86 vs. ARM: Mathá, Roland, et al. "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2 Instances." 2021 IEEE 17th International Conference on eScience (eScience). IEEE, 2021.
* FFMPEG H.264 video encoding guide: https://trac.ffmpeg.org/wiki/Encode/H.264
* Video quality measurement (old): https://en.wikipedia.org/wiki/Visual_information_fidelity (python package: https://pypi.org/project/sewar/)
* More video quality measurements (and benchmark code): https://piq.readthedocs.io/en/latest/overview.html
* FFMPEG profiling tool (contains only Docker containers): https://github.com/mboussaa/ffmpeg-profiling-tool
* Timeseries database for storing metrics: https://www.influxdata.com/influxdb-pricing/
* Observability of databases: https://grafana.com/
* Commercial encoding: https://www.telestream.net/
* pyTranscoder for FFMPEG (automate transcoding for people encoding lots of video. It is more than a wrapper - it is a workflow and job manager): https://pypi.org/project/pytranscoder-ffmpeg/
* Linux app profiling: https://stackoverflow.com/questions/2229336/linux-application-profiling
* FFMPEG quality cheatsheets: https://superuser.com/questions/490683/cheat-sheets-and-presets-settings-that-actually-work-with-ffmpeg-1-0
* FFMPEG Python bindings: https://kkroening.github.io/ffmpeg-python/
* Transcoding explanation: https://medium.com/videocoin/what-is-video-transcoding-and-why-do-you-do-it-348a2610cefc
* Effect of CRF on Youtube video quality: https://www.dropbox.com/s/fsxew599dw30c5w/Report.docx?dl=0
* Film making terms glossary: https://www.studiobinder.com/blog/movie-film-terms/
* Visual information fidelity: https://github.com/pavancm/Visual-Information-Fidelity---Python
* Image quality metrics (including VIFP) Python library (https://pypi.org/project/sewar/)
* Objective video quality assessment: Zhou Wang, Hamid R. Sheikh and Alan C. Bovik. Chapter 41 in The Handbook of Video Databases: Design and Applications, B. Furht and O. Marqure, ed., CRC Press, pp. 1041-1078, September 2003
* Video quality assessment: Netflix / vmaf: Video Multi-Method Assessment Fusion: https://en.wikipedia.org/wiki/Video_Multimethod_Assessment_Fusion
* Capped CRF: https://streaminglearningcenter.com/encoding/saving-encoding-streaming-deploy-capped-crf.html
* NVIDIA transcoding guide: https://developer.nvidia.com/blog/nvidia-ffmpeg-transcoding-guide/
* Process info (from Python): https://stackoverflow.com/questions/16326529/python-get-process-names-cpu-mem-usage-and-peak-mem-usage-in-windows
* ps CPU time measurement vs. top: https://unix.stackexchange.com/questions/460832/how-does-ps-measure-cpu-per-process-and-can-this-be-changed
* CPU time using atop and SAR: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-monitor-stats-with-atop/
* Various profilers: https://unix.stackexchange.com/questions/554/how-to-monitor-cpu-memory-usage-of-a-single-process
* More CPU usage: https://stackoverflow.com/questions/1221555/retrieve-cpu-usage-and-memory-usage-of-a-single-process-on-linux
* Optimal bit rate: https://teradek.com/blogs/articles/what-is-the-optimal-bitrate-for-your-resolution
* Bit rate tables: https://filmora.wondershare.com/video-editing-tips/what-is-video-bitrate.html
* Artifact: https://en.wikipedia.org/wiki/Artifact_(error). https://en.wikipedia.org/wiki/Ringing_artifacts
* Post processing filter: https://trac.ffmpeg.org/wiki/Postprocessing (Postprocessing is usually only useful with older codecs. Newer ones (including h.264, h.265, and VP8/9) all include deblocking filters as part of the codec. Most of the time it won't help to postprocess h.264, HEVC, VP8, or VP9 video. Postprocessing IS certainly useful for artifacty MPEG2 and MPEG4-ASP (divx, xvid), and even older codecs, as their output is often full of blocking and ringing when the encoder didn't have enough bitrate to make the output look good http://underpop.online.fr/f/ffmpeg/help/deblock.htm.gz)
* Setting H.264 profile (not necessary, as per https://trac.ffmpeg.org/wiki/Encode/H.264): https://superuser.com/questions/563997/how-can-i-set-a-h-264-profile-level-with-ffmpeg
* Profiling Python code: https://stackoverflow.com/questions/582336/how-do-i-profile-a-python-script
* Profiling FFMPEG using C++: https://ffmpeg-devel.ffmpeg.narkive.com/I8tuBDvU/profiling-the-library
* Profiling from Python: https://psutil.readthedocs.io/en/latest/index.html?highlight=oneshot#psutil.Process.oneshot
* Learning video quality metrics: https://www.amazon.com/gp/product/0998453005/ref=as_li_tl?ie=UTF8&tag=s0b649-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0998453005&linkId=aefb28b2d5273796a0d561808f34efbe
* Learning FFMPEG: https://streaminglearningcenter.com/learnffmpeg
* How to build your own cloud encoder: https://www.youtube.com/watch?v=mbaXINHo1Ys&feature=youtu.be
* https://superuser.com/questions/489087/what-are-the-differences-between-h-264-profiles/489092#489092
* Video compression picture types: https://en.wikipedia.org/wiki/Video_compression_picture_types#Bi-directional_predicted_frames.2Fslices_.28B-frames.2Fslices.29

Terms (sourced from the sites referenced above):
---
* Transcoding (which is a process of decoding, reformatting and re-encoding files) takes source footage of various types and recodes it into a single video codec or file format. Video transcoding, sometimes called video encoding, is the conversion from one digital encoding format to another. This involves translating all three elements of a digital video at the same time — the file format, the video, and the audio. It involves a two-step process. First, the original file is decoded to an uncompressed format. Second, this uncompressed format is then encoded into the target format.
* Video format specifies how video and audio have been combined and tells playback devices how to play the files.
* Video and audio codec formats refer to the technologies used to both create and play back the digital video and audio. DaCast, for example, requires an MPEG-4 (MP4) file format with an H.264 video codec. Windows Media Video (WMV) files are Advanced Systems Format (.asf) files that include audio, video, or both compressed with Windows Media Audio (WMA) and Windows Media Video (WMV) codecs.
* CRF: Constant rate factor (CRF) is an encoding mode that adjusts the file data rate up or down to achieve a selected quality level rather than a specific data rate. CRF values range from 0 to 51, with lower numbers delivering higher quality scores. Multiple codecs support CRF, including x264, x265, and VP9.
* A keyframe, also written as “key frame,” is something that defines the starting and/or ending point of any smooth transition. That something can be a drawing in animation or a particular frame of a shot when dealing with film or video. Any shot, animated or live-action, is broken down into individual frames. You can think of keyframes as the most important frames of a shot that set the parameters for the other frames and indicate the changes that will occur throughout as transitions. For more information, refer to our filmmaker’s guide to frame rates. Keyframe Characteristics: 1. Important individual frames from within a shot; 2. Sets a start/stop point for a transition. Because of the heavy demands and time-consuming nature of animation, those films are typically made by a number of artists working together. One efficient way to both save time and ensure quality is to have the lead animators draw the most important frames and leave the transitional frames between them to the junior animators. These important frames drawn by the lead animators became known as keyframes. The transitional frames that connected the various keyframes together become known as in-betweens. As used in the context of video editing, keyframes might set the parameters for motion, they might be used to fine-tune a video transition. Or they might control timed adjustments made to effects applied to a video. The editing software can fill in all of those pesky in-betweens automatically.
* Capped CRF: On its own, CRF is unusable for adaptive bitrate streaming, where data rates in the ladder rungs need to be limited. However, by adding a “cap” to CRF, you limit the data rate to that cap. An FFmpeg argument implementing capped CRF would look like this: `ffmpeg -i input_file -crf 23 -maxrate 6750k -bufsize 6750k output_file`. This tells FFmpeg to encode at a quality level of 23, but to cap the data rate at 6750 kbps with a VBV buffer of 4500 kbps. For easy-to-encode clips, the CRF value would limit the data rate, as the required quality could be achieved at data rates lower than the cap. For hard-to-encode clips, the cap would kick in to control the data rate.
* Block-ing: As the compression increases, there's more possibility of artifacts. The blocky artifact you see above (and in the images below) is called macroblocking. It's an artifact that happens when the video codec (encoder/decoder, like MPEG) can't handle the amount of information being thrown at it, at the chosen bit rate, and the result is blocks in the image. Why blocks? It's a result of how the image is compressed in the codec. You'll most commonly see macroblocking in fast motion, or when there's a lot going on in the image. The confetti at the end of the Superbowl was a macroblocking mess. Shots of the ocean are another problem, as there are a lot of individual things moving in the shot (like the wave tops). The macroblock is a processing unit in image and video compression formats based on linear block transforms, typically the discrete cosine transform (DCT). A macroblock typically consists of 16×16 samples, and is further subdivided into transform blocks, and may be further subdivided into prediction blocks. Formats which are based on macroblocks include JPEG, where they are called MCU blocks, H.261, MPEG-1 Part 2, H.262/MPEG-2 Part 2, H.263, MPEG-4 Part 2, and H.264/MPEG-4 AVC. In H.265/HEVC, the macroblock as a basic processing unit has been replaced by the coding tree unit.
* A deblocking filter is a video filter applied to decoded compressed video to improve visual quality and prediction performance by smoothing the sharp edges which can form between macroblocks when block coding techniques are used. The filter aims to improve the appearance of decoded pictures. 
* pixelation (or pixellation in British English) is caused by displaying a bitmap or a section of a bitmap at such a large size that individual pixels, small single-colored square display elements that comprise the bitmap, are visible. Such an image is said to be pixelated.
* A compression artifact (or artefact) is a noticeable distortion of media (including images, audio, and video) caused by the application of lossy compression. 
* In natural science and signal processing, an artifact or artefact[1] is any error in the perception or representation of any information introduced by the involved equipment or technique(s)
* Transmission errors: Data errors in the compressed bit-stream, possibly due to transmission errors, can lead to errors similar to large quantization errors, or can disrupt the parsing of the data stream entirely for a short time, leading to "break-up" of the picture. 
* Block boundary discontinuities can occur at edges of motion compensation prediction blocks. In motion compensated video compression, the current picture is predicted by shifting blocks (macroblocks, partitions, or prediction units) of pixels from previously decoded frames. If two neighboring blocks use different motion vectors, there will be a discontinuity at the edge between the blocks. 
* Mosquito noise: Block boundary discontinuities can occur at edges of motion compensation prediction blocks. In motion compensated video compression, the current picture is predicted by shifting blocks (macroblocks, partitions, or prediction units) of pixels from previously decoded frames. If two neighboring blocks use different motion vectors, there will be a discontinuity at the edge between the blocks. 
* Ringing artifact: In signal processing, particularly digital image processing, ringing artifacts are artifacts that appear as spurious signals near sharp transitions in a signal. Visually, they appear as bands or "ghosts" near edges; audibly, they appear as "echos" near transients, particularly sounds from percussion instruments; most noticeable are the pre-echos. 
* Encoding only audio: `ffmpeg -i in.mp4 -c:v copy -c:a aac out.mp4`. Video is only copied as-is. Audio is encoded. (https://superuser.com/questions/1196107/ffmpeg-low-cpu-usage)
* Classification of objective video quality models: 1. Full Reference Methods (FR): FR models compute the quality difference by comparing
the original video signal against the received video signal. Typically, every pixel from the
source is compared against the corresponding pixel at the received video. 2. Reduced Reference Methods (RR): RR models extract some features of both videos and compare them to give a quality score. They are used when all the original video is not available, or when it would be practically impossible to do so, e.g. in a transmission with a limited bandwidth. 3. No-Reference Methods (NR): NR models try to assess the quality of a distorted video without any reference to the original signal ((a)Pixel-Based Methods (NR-P): Pixel-based models use a decoded representation of the signal and analyze the quality based on the pixel information. Some of these evaluate specific degradation types only, such as blurring or other coding artifacts. (b) Parametric/Bitstream Methods (NR-B): These models make use of features extracted from the transmission container and/or video bitstream, e.g. MPEG-TS packet headers, motion vectors and quantization parameters. They do not have access to the original signal and require no decoding of the video, which makes them more efficient. In contrast to NR-P models, they have no access to the final decoded signal. However, the picture quality predictions they deliver are not very accurate. (c) Hybrid Methods (Hybrid NR-P-B): Hybrid models combine parameters extracted from the bitstream with a decoded video signal. They are therefore a mix between NR-P and NR-B models).
* Bitrate is the number of bits per second. A video bitrate is the number of bits that are processed in a unit of time. The symbol is bit/s. It generally determines the size and quality of video and audio files: the higher the bitrate, the better the quality, and the larger the file size because of File size = bitrate (kilobits per second) x duration. In most cases, 1 byte per second (1 B/s) corresponds to 8 bit/s.
* How to find the optimal resolution to bitrate ratio? The truth is, there really is no right or wrong answer. Depending on your encoder, video content, audience, and streaming destination, your resolution and bitrate will likely be different. Key Considerations for Choosing a Resolution and Bitrate: 1. How much bandwidth do I have to broadcast? Dedicate 50% of your upload speed to your stream in case your connection suddenly becomes unstable or you encounter unexpected interference from sending across the internet to the streaming platform. If you have limited bandwidth at your broadcast site but your encoder is capable of streaming with a more efficient video codec like HEVC / H.265 instead of just H.264, you can send out a lower bitrate HEVC stream from your encoder and have it transcoded to H.264 in the cloud for final delivery to streaming destinations and viewers. 2. How is my audience watching? How much bandwidth do they have? Consider where and how your audience will be watching your video and what their bandwidth limitations may be. Are they on computers or mobile phones? Using Ethernet or LTE? If the majority of your audience doesn’t have enough bandwidth to watch your video in 1080p, then you shouldn’t have to push your bandwidth over the edge just to upload a 1080p video. Cloud transcoding can help those watching with slower connections or on mobile devices with data limits because it enables the single stream sent out from your encoder to be converted into several lower resolution and lower bitrate streams that the viewers can choose instead of each viewer receiving the same higher resolution / higher bitrate stream. 3. What type of content am I sending? How much do I need to send? More dynamic content requires higher bitrates to have good quality, so you will need a higher bitrate to stream sporting events or video game competitions as opposed to speakers giving presentations at a conference or commencement ceremonies. The encoded frame rate also affects the required bitrate. When streaming sports you could encode a 60 frames per second (fps) stream at 1080p60 or 720p60; for lower motion events like lectures or conferences, encoding and sending 60 fps may not provide a visible benefit, but requires significantly more bandwidth than streaming at the more common 30 fps. Not all online streaming platforms can accept a 60 fps stream, and not all viewers can handle receiving 60 fps, so be sure to take that into consideration. Finally, there are two different methods of encoding the video: constant and variable. A constant bitrate encodes at approximately the same rate throughout the stream. While this can help maintain consistent quality, a constant bitrate isn’t always ideal for streaming over the internet since the same amount of data is being sent even when the content isn’t very complex, incurring higher costs from mobile data plans. 4. To which CDN / OVP am I streaming? If you're streaming to a single destination - direct to Facebook Live or YouTube Live, for instance - then your encoder’s settings need to match that destination’s requirements for bitrate and resolution on the incoming stream. If you're using Core or Sharelink to stream to multiple destinations at the same time, then you need to choose encoding settings that are compatible with every destination, since the encoder will only send out a single stream into Core or Sharelink, and that stream is then replicated to each destination. Keep in mind that every streaming platform comes with different presets which may limit the video bitrate and resolution combinations they will accept.
* What’s the difference between Constant Bitrate (CBR) and Variable Bitrate (VBR) Encoding. Constant bitrate (CBR) encoding maintains a set bitrate over the entire video clip, but limits the image quality in most cases—especially for complex video segments. CBR is often not an optimal choice for streaming since it does not allocate enough data for the complex segments: this results in lower quality overall and unused capacity on the simple segments. Therefore, we recommend you avoid using CBR unless you have a specific requirement.
* Resolution is the number of pixels spread across a display and is usually written in the form of horizontal pixels x vertical pixels, such as 1920 x 1080. The resolution of your display affects the highest resolution of stream you can watch. Resolution is sometimes referred to in a shorthand format using just the vertical pixels, such as 720p instead of 1280x720.
* Network bonding: If your available bandwidth isn’t robust enough to support streaming at your desired resolution and bitrate, you can use network bonding to combine bandwidth from multiple internet sources into a single, stronger connection. 
* H.264 (=AVC=Advanced Video Coding) processes frames of video using macroblocks, while H.265 (=HEVC=High Efficiency Video Coding) processes information using coding tree units (CTUs). CTUs process information more efficiently, which results in a smaller file size and less bandwidth used for your streamed video.
* Encoder settings: Protocol, video codec, framerate, keyframe frequency, audio codec, bitrate encoding, pixel aspect ratio, frame types, entropy encoding, audio sample rate, audio bitrate. (obtained from: https://support.google.com/youtube/answer/2853702?hl=en)
* Video Multimethod Assessment Fusion (VMAF) is an objective full-reference video quality metric. It predicts subjective video quality based on a reference and distorted video sequence. The metric can be used to evaluate the quality of different video codecs, encoders, encoding settings, or transmission variants. 
* Profile: A H.264 profile more or less defines what "bells and whistles" the encoder can use when compressing your video – and there are lots of H.264 features that the encoder can enable. Which ones it's allowed to enable is defined by the profile. Profiles ensure compatibility between devices that have different decoding capabilities. With profiles, the encoder and decoder agree on a feature set that they can both handle.
* I‑frames are the least compressible but don't require other video frames to decode.
* P‑frames can use data from previous frames to decompress and are more compressible than I‑frames.
* B‑frames can use both previous and forward frames for data reference to get the highest amount of data compression.
* A frame is a complete image.
* a field is the set of odd-numbered or even-numbered scan lines composing a partial image. For example, an HD 1080 picture has 1080 lines (rows) of pixels. An odd field consists of pixel information for lines 1, 3, 5...1079. An even field has pixel information for lines 2, 4, 6...1080. When video is sent in interlaced-scan format, each frame is sent in two fields, the field of odd-numbered lines followed by the field of even-numbered lines. 
* A frame used as a reference for predicting other frames is called a reference frame. 
* Picture: While the terms "frame" and "picture" are often used interchangeably, the term picture is a more general notion, as a picture can be either a frame or a field.
* "level" is a specified set of constraints that indicate a degree of required decoder performance for a profile. For example, a level of support within a profile specifies the maximum picture resolution, frame rate, and bit rate that a decoder may use. A decoder that conforms to a given level must be able to decode all bitstreams encoded for that level and all lower levels


Improvements:
---
* Asking about the end user or use case, to be able to choose the right parameters for encoding.
* Hiring video consultants for some training to build in-house expertise.
* Trying software like Sorenson and others, to check for better encoding capability.
* Building in-house knowledge by automating software that tries out various parameters and outcomes.
* Use VBR, increase the number of passes (at the expense of longer processing time)
* Audio: 128kbps or higher is fine. Could stereo to mono in some cases, but cutting down audio is negligible.
* Using a formula to get an estimated file size or encoding time. Could also build this knowledge in tables, as we process each video.
* GPU

Questions:
---
* Does audio lag happen after encoding? Under what situations?
* Paradox: H.265 will use more CPU when you try to stream in H.265 from a computer. However, it will give you higher quality at lower network speed.
* The commercial use of patented H.264 technologies requires the payment of royalties to MPEG LA and other patent owners. MPEG LA has allowed the free use of H.264 technologies for streaming Internet video that is free to end users. Does commercial use of FFMPEG require paying royalties for any situation?

Observations about the videos:
---
* The video player matters. Linux's default video player ends up having jitter during scenes where the camera flies across a geography. No such issue observed with VLC.


